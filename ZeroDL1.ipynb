{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 「ゼロから作るDeep Learning――Pythonで学ぶディープラーニングの理論と実装」の要点\n",
    "ディープラーニングのフレームワークを知りたかったので、シリーズの[ゼロから作るDeep Learning ❸――フレームワーク編](https://www.oreilly.co.jp/books/9784873119069/)を最初に読みました。次に、[ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)を読むと、❸には書かれてなかった理論的な解説などがなされていました。Deep Learningの基礎理論を学ぶのに大変良い本と思いました。その要点を記録していきます。\n",
    "\n",
    "### 参考文献\n",
    " - [O'Reilly Japan - ゼロから作るDeep Learning――Pythonで学ぶディープラーニングの理論と実装](https://www.oreilly.co.jp/books/9784873117584/)\n",
    " - [ゼロから作るDeep Learning――Pythonで学ぶディープラーニングの理論と実装｜Amazon](https://www.amazon.co.jp/dp/4873117585/)\n",
    " - [GitHub - oreilly-japan/deep-learning-from-scratch](https://github.com/oreilly-japan/deep-learning-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「1章 Python入門」\n",
    " - 参考：[Matplotlib: Python plotting — Matplotlib documentation](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「2章　パーセプトロン」\n",
    " - XORゲートは単層のパーセプトロンでは表せない。２層パーセプトロンで具体的にXORゲートを構成。\n",
    " - NANDゲートだけから（つまりパーセプトロンだけから）コンピュータを表現できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「3章　ニューラルネットワーク」\n",
    " - 単純パーセプトロンの活性化関数を0, 1を返すステップ関数からシグモイド関数のようななめらかな関数にして、更に多層にしたものがニューラネットワーク（「多層パーセプトロン」とも呼ばれる）。\n",
    " - ソフトマックス関数は平行移動しても値が変わらないので、指数関数が大きい値になり過ぎないよう平行移動する。\n",
    " - クラス分類の推論では、指数関数は単調関数で結果は同じなので、出力層のソフトマックス関数は省略するのが一般的。\n",
    " - pickle：オブジェクトをフアイルとして保存する機能。保存したplckleファイルをロードするとオブジェクトを復元できる。\n",
    " - `Image.fromarray()`：NumPyとして格納された画像データを、PIL用のデータオブジェクトに変換。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「4章　ニューラルネットワークの学習」\n",
    " - 参考：[random --- 擬似乱数を生成する](https://docs.python.org/ja/3/library/random.html)\n",
    " - 次は、NumPyの`np.random.choice`で選択した0から9の数字の列を、[oreilly-japan/deep-learning-from-scratch/dataset/mnist.py](https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/dataset/mnist.py)の`_change_one_hot_label`関数で**one-hot**表現に変換するサンプル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6 0 8 2]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "    return T\n",
    "\n",
    "choices = (np.random.choice(10, 5))\n",
    "print(choices)\n",
    "\n",
    "print(_change_one_hot_label(np.array(choices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「5章　誤差逆伝播法」\n",
    " - 計算グラフと連鎖律と誤差逆伝播法の詳細解説。\n",
    " - ２乗和誤差、ソフトマックス・交差エントロピー誤差の逆伝搬がキレイな式$\\left(y_{1}-t_{1},y_{2}-t_{2},y_{3}-t_{3}\\right)$になる。\n",
    " - ニューラルネットワークのレイヤを[`collections.OrderedDict`](https://docs.python.org/ja/3/library/collections.html#ordereddict-objects)として保持。\n",
    "   - `collections.OrderedDict`は、ディクショナリへの要素の挿入順序を保持。\n",
    "   - Pythonドキュメントによると、Python 3.7からは組み込みの`dict`クラスも挿入順序の記録が保証される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「6章　学習に関するテクニック」\n",
    " - 最適化手法SGD, Momentum, AdaGrad、RMSProp、Adamの詳しい解説。\n",
    " - Weight decay(荷重減衰)：過学習を抑え汎化性能を高めるよう重みが小さくなるように損失関数に重みのL2ノルムを加算する。\n",
    " - アクティベーション：活性化関数の後の出力データ、適度な広がりがある分布が良い。\n",
    " - 重みの初期値は、活性化関数が、Sigmoid関数、tanh関数の場合は**Xavierの初期値**が適しており、ReLUの場合は、**Heの初期値**が適している。\n",
    "   - Xavierの初期値：前層がノード数$n$の場合、標準偏差$\\frac{1}{\\sqrt{n}}$の分布\n",
    "   - Heの初期値：前層がノードの数$n$の場合、標準偏差$\\sqrt{\\frac{2}{n}}$のガウス分布\n",
    "   - DeZeroのソースを見ると初期値の分布を指定する機能はない：[oreilly-japan/deep-learning-from-scratch-3/dezero/layers.py](https://github.com/oreilly-japan/deep-learning-from-scratch-3/blob/master/dezero/layers.py)\n",
    " - バッチ正規化（Batch Norm[alization]）：ミニバッチごとにデータの分布が平均0で分散1になるように活性化関数の前（後）に正規化を行う（そしてスケール・シフトの変換を行う）。\n",
    "   - バッチ正規化の利点：学習が早く、初期値への依存が減り、過学習を抑制する。\n",
    " - ハイパーパラメータ最適化の際は、過学習しないように、訓練データとテストデータに加えて、検証データを用意する。グリッドサーチのような規則的なサンプリングよりもランダムサンプリングの方が効率的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「7章　畳み込みニューラルネットワーク」\n",
    " - 入力`(H, W)`、フィルター`(FH, FW)`、出カ`(OH, OW)`、パディング`P`、ストライド`S`とすると、$OH=\\frac{H+2P-FH}{S}+1$、$OW=\\frac{W+2P-FW}{S}+1$\n",
    " - 入力`(C, H, W)`にフィルター`(C, FH, FW)`を畳み込み`*`し、出力`(1, OH, OW)`を得る：`(C, H, W) * (C, FH, FW)` → `(1, OH, OW)`\n",
    "   - 出力はチャネル`C`個の和になる。\n",
    " - フィルターを`FN`個用意して出力を`FN`個得る：`(C, H, W) * (FN, C, FH, FW)` → `(FN, OH, OW)`\n",
    " - `FN`個のバイアス`(FN, 1, 1)`はブロードキャストされ上記出力に加算され最終出力になる。\n",
    " - バッチ`N`個としてまとめると：`(N, C, H, W) * (FN, C, FH, FW) + (FN, 1, 1)` → `(N, FN, OH, OW)`\n",
    " - `im2col`の解説は❸の方がわかりやすい。バッチ1個、出力1枚の場合をまとめる：\n",
    "   - 入力`(C, H, W)`から`im2col`でフィルター適用領域を一行にした行列`(OH*OW, C*FH*FW)`を作る。\n",
    "   - フィルターを一列にする`(C*FH*FW, 1)`\n",
    "   - 行列積で出力`(OH*OW, 1)`が得られ、`(OH, OW)`に`reshape`する。\n",
    " - Convolutionレイヤの逆伝播には、`im2col`の逆の処理を`col2im`関数を用いて行う。\n",
    "   - DeZero：[deep-learning-from-scratch-3/dezero/functions_conv.py](https://github.com/oreilly-japan/deep-learning-from-scratch-3/blob/master/dezero/functions_conv.py)\n",
    " - プーリングは、チャンネル毎に行われ、入力と出力のチャンネル数は同じ。学習パラメータはない。プーリングの解説も❸の方がわかりやすい。\n",
    " - CNNの実装例：フィルター数`30`サイズ`5`、ストライド`1`、パディング`0`。30のフィルターが学習前後で可視化。\n",
    " - ❸を読んで、自分で全結合MNISTをCNN化したときは、フィルター数`1`サイズ`3`、ストライド`1`、パディング`1`としたが、フィルター数はエッジの特徴を捉えられるように多くするべきだった。\n",
    " - LeNetとAlexNetの詳細解説。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「8章　ディープラーニング」\n",
    " - 次のようなディープなCNNが実装されている。\n",
    "   - 最適化手法：Adam、重みの初期値はHeの初期値\n",
    "   - `Conv(チャンネル数, フィルターサイズ)`\n",
    "\n",
    "```\n",
    "Conv(16, 3)->ReLU->C(16, 3)->R->Pool->C(32, 3)->R->C(32, 3)->R->P->\n",
    "C(64, 3)->R->C(64, 3)->R->P->Affine->R->Dropout->A->D->Softmax\n",
    "```\n",
    " - アンサンブル学習、学習係数の減衰、データ拡張などは、認識精度の向上に有効\n",
    " - データ拡張：入力画像の拡張（回転・平行移動・crop・flip・輝度変化・拡大・縮小など）\n",
    " - ディープ化が有効である例：Conv5, 7よりConv3をそれぞれ2, 3回する方がパラメータが減り表現力が高まる。\n",
    " - VGG、GoogLeNet、ResNet、転移学習・再学習、GPU・分散学習、物体検出、セグメンテーション、画像キャプション生成、画像スタイル変換、画像生成、自動運転、強化学習などについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「付録A　Softmax-with-Lossレイヤの計算グラフ」\n",
    " - 計算の詳細"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
