{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeZeroのMNISTに畳み込み層とプーリング層とDropoutを追加して実験\n",
    "「[ゼロから作るDeep Learning ❸](https://www.oreilly.co.jp/books/9784873119069/)ステップ51 MNISTの学習」の[`deep-learning-from-scratch-3/examples/mnist.py`](https://github.com/oreilly-japan/deep-learning-from-scratch-3/blob/master/examples/mnist.py)は、次のようなネットワーク構成になっています。\n",
    "\n",
    " - `(1, 28, 28)入力→(784,)配列→Linear(1000)→ReLU→Linear(1000)→ReLU→Linear(10)出力`\n",
    " - 最適化手法：Adam\n",
    "\n",
    "学習経過は次のようになります。\n",
    "\n",
    "```bash\n",
    "$ python -m examples.mnist\n",
    "epoch: 1\n",
    "train loss: 0.19297177767846732, accuracy: 0.9425166666666667\n",
    "test loss: 0.0870012722350657, accuracy: 0.972\n",
    "epoch: 2\n",
    "train loss: 0.07911744002679673, accuracy: 0.9756333333333334\n",
    "test loss: 0.08153347594663501, accuracy: 0.9735\n",
    "epoch: 3\n",
    "train loss: 0.05772087200079113, accuracy: 0.9810833333333333\n",
    "test loss: 0.06439740799571154, accuracy: 0.9804\n",
    "epoch: 4\n",
    "train loss: 0.04381801278757242, accuracy: 0.9858\n",
    "test loss: 0.07971655959896452, accuracy: 0.9763\n",
    "epoch: 5\n",
    "train loss: 0.03815114084631205, accuracy: 0.9882666666666666\n",
    "test loss: 0.07151200462241832, accuracy: 0.9778\n",
    "```\n",
    "\n",
    "これに次のような、標準的な畳み込み層、プーリング層、ドロップアウトを追加したモデルを作成します。\n",
    "\n",
    " - `(1, 28, 28)入力→Conv→ReLU→Pool(2)→Linear(1000)→ReLU→Dropout→Linear(1000)→ReLU→Dropout→Linear(10)出力`\n",
    "   - `Conv`は、カーネルサイズ=3、ストライドサイズ=1、パディングサイズ=1\n",
    "   - 入力データサイズ28 + 2 * パディングサイズ1 - カーネルサイズ3 // ストライドサイズ1 + 1 = 出力サイズ28\n",
    "   - 2x2プーリングなので全結合層`Linear(1000)`に入る時点で縦横半分の14x14となる。\n",
    "\n",
    "実行は、`pip install dezero`（Jupyterなら`!pip install dezero`）でDeZeroをインストールして次を実行するか、\n",
    "[`qitqito/dezero_study/mnist_plus.py`](https://github.com/qitqito/dezero_study/blob/master/mnist_plus.py)をダウンロードして`python -m mnist_plus`などして下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 3.2967599945267043, accuracy: 0.7726333333333333\n",
      "test loss: 0.6105298531055451, accuracy: 0.841\n",
      "epoch: 2\n",
      "train loss: 0.4918030514443914, accuracy: 0.8647833333333333\n",
      "test loss: 0.39708854403346777, accuracy: 0.8898\n",
      "epoch: 3\n",
      "train loss: 0.365787100456655, accuracy: 0.8968166666666667\n",
      "test loss: 0.3391792545514181, accuracy: 0.9072\n",
      "epoch: 4\n",
      "train loss: 0.31472394374509655, accuracy: 0.9104833333333333\n",
      "test loss: 0.33774808773770926, accuracy: 0.9031\n",
      "epoch: 5\n",
      "train loss: 0.2928329927722613, accuracy: 0.9176166666666666\n",
      "test loss: 0.30439508248120545, accuracy: 0.9157\n",
      "epoch: 6\n",
      "train loss: 0.2737082013487816, accuracy: 0.9220666666666667\n",
      "test loss: 0.2845079012773931, accuracy: 0.9228\n",
      "epoch: 7\n",
      "train loss: 0.24680014058947564, accuracy: 0.92935\n",
      "test loss: 0.28378744328394534, accuracy: 0.9223\n",
      "epoch: 8\n",
      "train loss: 0.23471303268025318, accuracy: 0.9321666666666667\n",
      "test loss: 0.27566407042555513, accuracy: 0.9266\n",
      "epoch: 9\n",
      "train loss: 0.21447662274663648, accuracy: 0.9387333333333333\n",
      "test loss: 0.2557909615244716, accuracy: 0.9328\n",
      "epoch: 10\n",
      "train loss: 0.17519887572464843, accuracy: 0.9480833333333333\n",
      "test loss: 0.20000756379682572, accuracy: 0.9445\n",
      "epoch: 11\n",
      "train loss: 0.1535407663229853, accuracy: 0.9543666666666667\n",
      "test loss: 0.17418874572496862, accuracy: 0.95\n",
      "epoch: 12\n",
      "train loss: 0.128863106208543, accuracy: 0.96085\n",
      "test loss: 0.14710670401575043, accuracy: 0.9567\n",
      "epoch: 13\n",
      "train loss: 0.1193117741898944, accuracy: 0.9644833333333334\n",
      "test loss: 0.13540442480705678, accuracy: 0.9611\n",
      "epoch: 14\n",
      "train loss: 0.11357438637875021, accuracy: 0.9650166666666666\n",
      "test loss: 0.14487418973236343, accuracy: 0.9601\n",
      "epoch: 15\n",
      "train loss: 0.10861809722458322, accuracy: 0.9663333333333334\n",
      "test loss: 0.14201724069891497, accuracy: 0.9611\n",
      "epoch: 16\n",
      "train loss: 0.10236691535140077, accuracy: 0.9684166666666667\n",
      "test loss: 0.14015006882196757, accuracy: 0.9598\n",
      "epoch: 17\n",
      "train loss: 0.10014666023974618, accuracy: 0.9696833333333333\n",
      "test loss: 0.14514980674837716, accuracy: 0.9618\n",
      "epoch: 18\n",
      "train loss: 0.09907583203942825, accuracy: 0.9695666666666667\n",
      "test loss: 0.1305210689175874, accuracy: 0.9628\n",
      "epoch: 19\n",
      "train loss: 0.10227264529559761, accuracy: 0.9690833333333333\n",
      "test loss: 0.14776304945116864, accuracy: 0.9622\n",
      "epoch: 20\n",
      "train loss: 0.09374648537564402, accuracy: 0.9714166666666667\n",
      "test loss: 0.13284143024939113, accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import DataLoader\n",
    "from dezero.models import Model\n",
    "import dezero.layers as L\n",
    "\n",
    "\n",
    "class MNISTPlus(Model):\n",
    "    def __init__(self, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = L.Conv2d(1, kernel_size=3, stride=1, pad=1)\n",
    "        #self.conv2 = L.Conv2d(1, kernel_size=3, stride=1, pad=1)\n",
    "        self.fc3 = L.Linear(hidden_size)\n",
    "        self.fc4 = L.Linear(hidden_size)\n",
    "        self.fc5 = L.Linear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 28x28\n",
    "        x = F.pooling(x, 2, 2) # 14x14\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.pooling(x, 2, 2)\n",
    "        x = F.reshape(x, (x.shape[0], -1)) # 14x14を196に\n",
    "        x = F.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "max_epoch = 20\n",
    "batch_size = 100\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None) # 28x28のまま\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=None) # 28x28のまま\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MNISTPlus(1000)\n",
    "optimizer = dezero.optimizers.Adam().setup(model)\n",
    "optimizer.add_hook(dezero.optimizers.WeightDecay(1e-4))  # Weight decay\n",
    "\n",
    "if dezero.cuda.gpu_enable:\n",
    "    train_loader.to_gpu()\n",
    "    test_loader.to_gpu()\n",
    "    model.to_gpu()\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {}, accuracy: {}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {}, accuracy: {}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "全結合層に畳み込み層・プーリング層・Dropoutを追加したら学習性能が悪化しました。\n",
    "\n",
    "MNISTの画像が小さ過ぎて畳み込みやプーリングが情報を減らしてしまうのかも知れません。Dropoutも有無で実験したところ学習を遅くしてるように思われます。全結合層は1000より小さくしても良いかも知れません。。。\n",
    "\n",
    "自分には、知識不足でろくな考察ができないと分かったので、当分は深層学習などを色々学んでいき、ブログ一記事分たまったらその都度書いていこうと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「ゼロから作るDeep Learning――Pythonで学ぶディープラーニングの理論と実装」読了後の追記\n",
    "「ゼロから作るDeep Learning」を読んで、畳み込み層のチャンネル数は、エッジを捉えられるくらいの多目の数にするのが良いことが分かったので、チャンネル数を30に変更して、全結合層は1000の2層から100の1層に減らした。\n",
    "\n",
    "前回の畳み込み層は余り機能してなくて、全結合層で殆どの学習が行われていたと思われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 2.6159827625751495, accuracy: 0.3653\n",
      "test loss: 1.5855809235572815, accuracy: 0.4245\n",
      "epoch: 2\n",
      "train loss: 1.3986184002955755, accuracy: 0.5139\n",
      "test loss: 1.2680810564756393, accuracy: 0.5488\n",
      "epoch: 3\n",
      "train loss: 1.2251906350255013, accuracy: 0.5772\n",
      "test loss: 1.1655157601833344, accuracy: 0.5798\n",
      "epoch: 4\n",
      "train loss: 1.0788773282368977, accuracy: 0.6312\n",
      "test loss: 1.0249468386173248, accuracy: 0.6461\n",
      "epoch: 5\n",
      "train loss: 0.9687912953893344, accuracy: 0.6698\n",
      "test loss: 0.905553662776947, accuracy: 0.6993\n",
      "epoch: 6\n",
      "train loss: 0.8521708323558171, accuracy: 0.7078833333333333\n",
      "test loss: 0.8258089834451675, accuracy: 0.7273\n",
      "epoch: 7\n",
      "train loss: 0.753289823482434, accuracy: 0.7476\n",
      "test loss: 0.7056331288814545, accuracy: 0.7729\n",
      "epoch: 8\n",
      "train loss: 0.6363991202414035, accuracy: 0.7902666666666667\n",
      "test loss: 0.6006712540984154, accuracy: 0.8134\n",
      "epoch: 9\n",
      "train loss: 0.5534438830117384, accuracy: 0.8214166666666667\n",
      "test loss: 0.5486635291576385, accuracy: 0.8303\n",
      "epoch: 10\n",
      "train loss: 0.48624121643602847, accuracy: 0.8448333333333333\n",
      "test loss: 0.4355174897611141, accuracy: 0.8624\n",
      "epoch: 11\n",
      "train loss: 0.38985034567614396, accuracy: 0.8732833333333333\n",
      "test loss: 0.3875940635800362, accuracy: 0.8782\n",
      "epoch: 12\n",
      "train loss: 0.33717630786200364, accuracy: 0.89085\n",
      "test loss: 0.33944033928215506, accuracy: 0.894\n",
      "epoch: 13\n",
      "train loss: 0.3001469767342011, accuracy: 0.90555\n",
      "test loss: 0.31345576196908953, accuracy: 0.9064\n",
      "epoch: 14\n",
      "train loss: 0.2569127837071816, accuracy: 0.9189\n",
      "test loss: 0.2859761542081833, accuracy: 0.9187\n",
      "epoch: 15\n",
      "train loss: 0.2331749682004253, accuracy: 0.9265833333333333\n",
      "test loss: 0.26578663393855095, accuracy: 0.9281\n",
      "epoch: 16\n",
      "train loss: 0.21333846520632505, accuracy: 0.9328166666666666\n",
      "test loss: 0.24413680125027895, accuracy: 0.9287\n",
      "epoch: 17\n",
      "train loss: 0.190314938214918, accuracy: 0.9402333333333334\n",
      "test loss: 0.2217635857500136, accuracy: 0.9409\n",
      "epoch: 18\n",
      "train loss: 0.17526820365339518, accuracy: 0.9465\n",
      "test loss: 0.20528865659609438, accuracy: 0.9427\n",
      "epoch: 19\n",
      "train loss: 0.15845421771829327, accuracy: 0.95105\n",
      "test loss: 0.2110242011770606, accuracy: 0.947\n",
      "epoch: 20\n",
      "train loss: 0.15129534992389382, accuracy: 0.9540666666666666\n",
      "test loss: 0.19538673297502102, accuracy: 0.9436\n",
      "epoch: 21\n",
      "train loss: 0.1299619611321638, accuracy: 0.9615666666666667\n",
      "test loss: 0.20359565939754248, accuracy: 0.9529\n",
      "epoch: 22\n",
      "train loss: 0.1239572739119952, accuracy: 0.9642166666666667\n",
      "test loss: 0.191039286730811, accuracy: 0.9551\n",
      "epoch: 23\n",
      "train loss: 0.10954953731192897, accuracy: 0.9676166666666667\n",
      "test loss: 0.1572392075881362, accuracy: 0.9616\n",
      "epoch: 24\n",
      "train loss: 0.09915241319608564, accuracy: 0.9703166666666667\n",
      "test loss: 0.17375590597017435, accuracy: 0.9573\n",
      "epoch: 25\n",
      "train loss: 0.09335310317420711, accuracy: 0.97215\n",
      "test loss: 0.16521035187179223, accuracy: 0.963\n",
      "epoch: 26\n",
      "train loss: 0.0857502267540743, accuracy: 0.97325\n",
      "test loss: 0.1612675786553882, accuracy: 0.9628\n",
      "epoch: 27\n",
      "train loss: 0.08284967424425607, accuracy: 0.9749833333333333\n",
      "test loss: 0.15760683533502742, accuracy: 0.9647\n",
      "epoch: 28\n",
      "train loss: 0.08424129726209988, accuracy: 0.9744\n",
      "test loss: 0.17341100088553504, accuracy: 0.9643\n",
      "epoch: 29\n",
      "train loss: 0.07548866084932039, accuracy: 0.9770666666666666\n",
      "test loss: 0.1684789330314379, accuracy: 0.9664\n",
      "epoch: 30\n",
      "train loss: 0.07535748413143059, accuracy: 0.9775\n",
      "test loss: 0.16175081465626134, accuracy: 0.9692\n",
      "epoch: 31\n",
      "train loss: 0.07321014858161409, accuracy: 0.9778333333333333\n",
      "test loss: 0.16738545139029157, accuracy: 0.9638\n",
      "epoch: 32\n",
      "train loss: 0.06938195702891486, accuracy: 0.9797833333333333\n",
      "test loss: 0.16690163615799974, accuracy: 0.9672\n",
      "epoch: 33\n",
      "train loss: 0.06407026944022315, accuracy: 0.9808666666666667\n",
      "test loss: 0.16426528005395086, accuracy: 0.9657\n",
      "epoch: 34\n",
      "train loss: 0.06622292538124991, accuracy: 0.98035\n",
      "test loss: 0.16677145272726193, accuracy: 0.9672\n",
      "epoch: 35\n",
      "train loss: 0.06239132017605395, accuracy: 0.9811833333333333\n",
      "test loss: 0.1584336415323196, accuracy: 0.9675\n",
      "epoch: 36\n",
      "train loss: 0.059627835177137364, accuracy: 0.9824166666666667\n",
      "test loss: 0.16518179552396758, accuracy: 0.9662\n",
      "epoch: 37\n",
      "train loss: 0.064749651090436, accuracy: 0.9803\n",
      "test loss: 0.1500025482138153, accuracy: 0.9708\n",
      "epoch: 38\n",
      "train loss: 0.061265331497997975, accuracy: 0.9820166666666666\n",
      "test loss: 0.13568101231125185, accuracy: 0.9671\n",
      "epoch: 39\n",
      "train loss: 0.05821487150038593, accuracy: 0.9829666666666667\n",
      "test loss: 0.15981507711519952, accuracy: 0.9716\n",
      "epoch: 40\n",
      "train loss: 0.05861250429326901, accuracy: 0.9830666666666666\n",
      "test loss: 0.15769984688144179, accuracy: 0.9697\n",
      "epoch: 41\n",
      "train loss: 0.0562597776891198, accuracy: 0.9836166666666667\n",
      "test loss: 0.16388860407983885, accuracy: 0.968\n",
      "epoch: 42\n",
      "train loss: 0.05517039467085851, accuracy: 0.9833\n",
      "test loss: 0.16911549092663336, accuracy: 0.9686\n",
      "epoch: 43\n",
      "train loss: 0.051467320154430736, accuracy: 0.9852\n",
      "test loss: 0.13788449094747193, accuracy: 0.972\n",
      "epoch: 44\n",
      "train loss: 0.050477623512852, accuracy: 0.9843666666666666\n",
      "test loss: 0.14787182633328486, accuracy: 0.9703\n",
      "epoch: 45\n",
      "train loss: 0.05482214416047403, accuracy: 0.9840333333333333\n",
      "test loss: 0.17122801423014608, accuracy: 0.9698\n",
      "epoch: 46\n",
      "train loss: 0.05231050335384983, accuracy: 0.984\n",
      "test loss: 0.15245031248428859, accuracy: 0.9703\n",
      "epoch: 47\n",
      "train loss: 0.05411478022676117, accuracy: 0.9840833333333333\n",
      "test loss: 0.15859288179985015, accuracy: 0.9676\n",
      "epoch: 48\n",
      "train loss: 0.05061238624538722, accuracy: 0.98495\n",
      "test loss: 0.17231930229230785, accuracy: 0.971\n",
      "epoch: 49\n",
      "train loss: 0.04995286160808367, accuracy: 0.9855666666666667\n",
      "test loss: 0.15316623038350372, accuracy: 0.9693\n",
      "epoch: 50\n",
      "train loss: 0.05111634426507711, accuracy: 0.98495\n",
      "test loss: 0.14023779671173542, accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import DataLoader\n",
    "from dezero.models import Model\n",
    "import dezero.layers as L\n",
    "\n",
    "\n",
    "class MNISTPlus(Model):\n",
    "    def __init__(self, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = L.Conv2d(30, kernel_size=3, stride=1, pad=1)\n",
    "        #self.conv2 = L.Conv2d(1, kernel_size=3, stride=1, pad=1)\n",
    "        self.fc3 = L.Linear(hidden_size)\n",
    "        #self.fc4 = L.Linear(hidden_size)\n",
    "        self.fc5 = L.Linear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 28x28\n",
    "        x = F.pooling(x, 2, 2) # 14x14\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.pooling(x, 2, 2)\n",
    "        x = F.reshape(x, (x.shape[0], -1)) # 14x14を196に\n",
    "        x = F.dropout(F.relu(self.fc3(x)))\n",
    "        #x = F.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "max_epoch = 50\n",
    "batch_size = 100\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None) # 28x28のまま\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=None) # 28x28のまま\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MNISTPlus()\n",
    "optimizer = dezero.optimizers.Adam().setup(model)\n",
    "optimizer.add_hook(dezero.optimizers.WeightDecay(1e-4))  # Weight decay\n",
    "\n",
    "if dezero.cuda.gpu_enable:\n",
    "    train_loader.to_gpu()\n",
    "    test_loader.to_gpu()\n",
    "    model.to_gpu()\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {}, accuracy: {}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {}, accuracy: {}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "学習に必要なエポック数は増えたが、精度は1%近く向上した。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
